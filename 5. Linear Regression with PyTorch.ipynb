{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. About Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Simple Linear Regression Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allows us to understand **relationship** between two **continuous variables**\n",
    "- Example\n",
    "    - x: independent variable\n",
    "        - weight\n",
    "    - y: dependent variable\n",
    "        - height\n",
    "    - y = $\\alpha x + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Example of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0795d7e6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.rand(n)\n",
    "\n",
    "colors = np.random.rand(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Aim of Linear Regression\n",
    "\n",
    "- Minimize the distance between the points and the line($y = \\alpha x + \\beta$)\n",
    "- Adjusting\n",
    "    - Coefficient: $\\alpha$\n",
    "    - Bias/intercept : $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building a Linear Regression Model with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coefficient: $\\alpha$ = 2\n",
    "- Bias/intercept: $\\beta$ = 1\n",
    "- Equation: $y = 2x + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Building a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: 2D required\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = [2*i + 1 for i in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: 2D required\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "1. Linear Model\n",
    "    - True Equation: $y = 2x + 1$\n",
    "2. Forward\n",
    "    - Example\n",
    "        - Input $x = 1$\n",
    "        - Output $\\hat{y}=?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insantiate Model Class\n",
    "- input : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- desired output : [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LinearRegressionModel(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE Loss : Mean Squared Error\n",
    "- $MSE = \\frac{1}{2}\\sum^{n}_{i=1}(\\hat{y_i} - y_i)$\n",
    "    - $\\hat{y}$: prediction\n",
    "    - $y$: true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instnatiate Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}$\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_{\\theta}$: parameters' gradients\n",
    "        \n",
    "    - Even simplier equation\n",
    "        - parameters = parameters - learning_rate * parameters_gradients\n",
    "            - parameters: $\\alpha$ and $\\beta$ in $y = \\alpha x + \\beta$\n",
    "            - desired parameters: $\\alpha = 2$ and $\\beta = 1$ in $y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "- 1 epoch: going through the whole x_train data once\n",
    "    - 100 epochs:\n",
    "        - 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- Process\n",
    "    1. Convert inputs/labes to variables\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs\n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t parameters\n",
    "    6. Update parameters using gradients\n",
    "        - parameters = parameters - learning_rate * parameters_gradients\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 210.58480834960938\n",
      "epoch 2, loss 17.561738967895508\n",
      "epoch 3, loss 1.8131591081619263\n",
      "epoch 4, loss 0.5243470668792725\n",
      "epoch 5, loss 0.4150182604789734\n",
      "epoch 6, loss 0.40194380283355713\n",
      "epoch 7, loss 0.39676710963249207\n",
      "epoch 8, loss 0.3922802209854126\n",
      "epoch 9, loss 0.3878951072692871\n",
      "epoch 10, loss 0.38356322050094604\n",
      "epoch 11, loss 0.3792799115180969\n",
      "epoch 12, loss 0.37504473328590393\n",
      "epoch 13, loss 0.37085655331611633\n",
      "epoch 14, loss 0.3667151629924774\n",
      "epoch 15, loss 0.3626200258731842\n",
      "epoch 16, loss 0.3585708737373352\n",
      "epoch 17, loss 0.3545668125152588\n",
      "epoch 18, loss 0.350607305765152\n",
      "epoch 19, loss 0.34669220447540283\n",
      "epoch 20, loss 0.3428206741809845\n",
      "epoch 21, loss 0.3389926254749298\n",
      "epoch 22, loss 0.3352069854736328\n",
      "epoch 23, loss 0.33146360516548157\n",
      "epoch 24, loss 0.32776251435279846\n",
      "epoch 25, loss 0.3241022229194641\n",
      "epoch 26, loss 0.32048314809799194\n",
      "epoch 27, loss 0.3169042766094208\n",
      "epoch 28, loss 0.3133653998374939\n",
      "epoch 29, loss 0.3098662197589874\n",
      "epoch 30, loss 0.3064059615135193\n",
      "epoch 31, loss 0.3029842674732208\n",
      "epoch 32, loss 0.29960110783576965\n",
      "epoch 33, loss 0.2962554693222046\n",
      "epoch 34, loss 0.2929472029209137\n",
      "epoch 35, loss 0.2896758019924164\n",
      "epoch 36, loss 0.28644105792045593\n",
      "epoch 37, loss 0.283242404460907\n",
      "epoch 38, loss 0.2800796329975128\n",
      "epoch 39, loss 0.2769518494606018\n",
      "epoch 40, loss 0.2738592326641083\n",
      "epoch 41, loss 0.2708010971546173\n",
      "epoch 42, loss 0.2677769958972931\n",
      "epoch 43, loss 0.26478689908981323\n",
      "epoch 44, loss 0.2618299424648285\n",
      "epoch 45, loss 0.2589062452316284\n",
      "epoch 46, loss 0.25601503252983093\n",
      "epoch 47, loss 0.25315630435943604\n",
      "epoch 48, loss 0.25032925605773926\n",
      "epoch 49, loss 0.24753384292125702\n",
      "epoch 50, loss 0.24476970732212067\n",
      "epoch 51, loss 0.2420363873243332\n",
      "epoch 52, loss 0.23933358490467072\n",
      "epoch 53, loss 0.23666097223758698\n",
      "epoch 54, loss 0.2340182662010193\n",
      "epoch 55, loss 0.23140496015548706\n",
      "epoch 56, loss 0.2288208156824112\n",
      "epoch 57, loss 0.22626565396785736\n",
      "epoch 58, loss 0.22373899817466736\n",
      "epoch 59, loss 0.22124043107032776\n",
      "epoch 60, loss 0.21876996755599976\n",
      "epoch 61, loss 0.21632696688175201\n",
      "epoch 62, loss 0.21391133964061737\n",
      "epoch 63, loss 0.21152263879776\n",
      "epoch 64, loss 0.20916056632995605\n",
      "epoch 65, loss 0.2068248689174652\n",
      "epoch 66, loss 0.20451530814170837\n",
      "epoch 67, loss 0.20223166048526764\n",
      "epoch 68, loss 0.19997334480285645\n",
      "epoch 69, loss 0.1977401226758957\n",
      "epoch 70, loss 0.19553206861019135\n",
      "epoch 71, loss 0.19334863126277924\n",
      "epoch 72, loss 0.1911894828081131\n",
      "epoch 73, loss 0.18905459344387054\n",
      "epoch 74, loss 0.18694353103637695\n",
      "epoch 75, loss 0.1848558485507965\n",
      "epoch 76, loss 0.18279153108596802\n",
      "epoch 77, loss 0.18075037002563477\n",
      "epoch 78, loss 0.17873188853263855\n",
      "epoch 79, loss 0.17673608660697937\n",
      "epoch 80, loss 0.17476242780685425\n",
      "epoch 81, loss 0.17281094193458557\n",
      "epoch 82, loss 0.17088119685649872\n",
      "epoch 83, loss 0.1689729243516922\n",
      "epoch 84, loss 0.16708610951900482\n",
      "epoch 85, loss 0.16522033512592316\n",
      "epoch 86, loss 0.16337524354457855\n",
      "epoch 87, loss 0.16155081987380981\n",
      "epoch 88, loss 0.15974687039852142\n",
      "epoch 89, loss 0.15796291828155518\n",
      "epoch 90, loss 0.15619918704032898\n",
      "epoch 91, loss 0.15445484220981598\n",
      "epoch 92, loss 0.15273000299930573\n",
      "epoch 93, loss 0.1510244756937027\n",
      "epoch 94, loss 0.1493380218744278\n",
      "epoch 95, loss 0.1476704329252243\n",
      "epoch 96, loss 0.14602141082286835\n",
      "epoch 97, loss 0.14439080655574799\n",
      "epoch 98, loss 0.14277847111225128\n",
      "epoch 99, loss 0.141184002161026\n",
      "epoch 100, loss 0.13960741460323334\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print (f'epoch {epoch}, loss {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3049524],\n",
       "       [ 2.4050455],\n",
       "       [ 4.505139 ],\n",
       "       [ 6.605232 ],\n",
       "       [ 8.705325 ],\n",
       "       [10.805418 ],\n",
       "       [12.905511 ],\n",
       "       [15.005605 ],\n",
       "       [17.105698 ],\n",
       "       [19.205791 ],\n",
       "       [21.305883 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purely inference\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = 2x + 1\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl0XOWZ5/Hvq7W0lkqLtVirsZEs2/KCYuyAwcSGkEBIx4HQmXaHJHSY6TSQZJowzPyTnD45p8kcBybnhE6G7hCgQ0inidNN93Qn2NiEADbGZnGMJe+yLNvay7K2kqpK7/yhBdlYWJZUdWv5fc7RUS1Xuk+V5Z+u3rr1PMZai4iIRL8EpwsQEZG5oUAXEYkRCnQRkRihQBcRiREKdBGRGKFAFxGJEQp0EZEYoUAXEYkRCnQRkRiRFM6d5efn28rKynDuUkQk6u3bt6/TWltwue3CGuiVlZXs3bs3nLsUEYl6xpiT09lOSy4iIjFCgS4iEiMU6CIiMSKsa+iX4vf7aWlpwefzOV1KTHO5XJSWlpKcnOx0KSISIo4HektLC1lZWVRWVmKMcbqcmGStpauri5aWFqqqqpwuR0RCxPElF5/PR15ensI8hIwx5OXl6a8gkRjneKADCvMw0HMsEvsiItBFRGLV4HCQ/qFAWPbl+Bq607q6utiwYQMAra2tJCYmUlAw+oasPXv2kJKSEpL9Xn/99fzoRz9ixYoVU27z2GOP8fWvfx2XyxWSGkQkdKy1HGnv4xdvvcfx3rdJSn+Xcnc5m2o2UVdUF5J9Rl2g72/dz9bGrTT3NM/Jk5OXl8e7774LwHe/+10yMzN56KGHLtjGWou1loSE8P5B89hjj/HVr35VgS4SZfqGAuxobOeN48fY1/4yC4r6KcwuxTvoZcuuLTy09qGQhHpULbnsb93Pll1b8A56KZ305Oxv3T/n+zp69Ci1tbX82Z/9GUuWLOHUqVPk5ORM3P/LX/6Sv/iLvwCgra2NTZs2UV9fz+rVq9m9e/eHvt/AwAB33XUXixcv5vOf//wFL1Ded9991NfXs2TJEv7mb/4GgMcff5z29nbWrVvHxo0bp9xORCLLUCDIz3ef5GRnP+d5k8VlXordGSSYBDxpHjwuD1sbt4Zk31F1hL61cSselwdPmgdg4vPWxq0h+W3X2NjIs88+S319PYHA1GtgDz74IA8//DBr1qyhqamJ22+/nQMHDlywzY9+9CM8Hg8NDQ2888471NfXT9z36KOPkpubSyAQ4KabbuLOO+/kW9/6Fj/4wQ/4wx/+MPGL5FLb1dbWzvnjFpErNzgcJC0lkdSkRNYtyqfEncZfbz9IaVrpBdu5XW6ae5pDUkNUBXpzTzOl2eF7cq666qoLgncq27dv59ChQxPXvV4vg4ODpKWlTdz26quv8vDDDwOwcuVKlixZMnHf888/z09/+lMCgQBnzpzh4MGDlwzq6W4nIuEzMmJ5t+Ucbxzt5DPLS6jIy2BJiRuAcnc53kHvxMEnQI+vh3J3eUhqiapAD/eTk5GRMXE5ISEBa+3E9clLJtbaGb+AeuTIEX74wx+yZ88ecnJy2Lx58yXPF5/udiISPl19Q2w72MbZHh8LCjLIzbgwAzbVbGLLri3A6MFnj68Hr8/LvSvvDUk9UbWGvqlmE16fF++glxE7gnfQi9fnZVPNppDvOyEhAY/Hw5EjRxgZGeE3v/nNxH0bN27kiSeemLg+/iLrZDfccAO/+MUvAHjvvfd4//33ATh//jxZWVlkZ2dz9uxZfve73018TVZWFr29vZfdTkTCb99JL8+92cy5QT+fWlbEHctLyHJd2FqjrqiOh9Y+hCfNQ8v5FjxpnpC9IApRdoQ+/uRMPsvl3pX3huzJudj3v/99PvnJTzJv3jyuueYahoaGAHjiiSf4y7/8S372s59NrG9PDniA+++/n3vuuYfFixezZMkSVq5cCcCqVauora2lpqaGiooKrrvuuomvue+++9i4cSNlZWVs27Ztyu1EJPySEw0L52WyvrqA9JSpo7SuqC5sGWUmLyOEWn19vb14wEVDQwOLFy8OWw3xTM+1yMz5gyPsPt5FbkYKS0rcWGvD9g5sY8w+a+1lX9C77JKLMabMGLPTGHPQGPO+MeYbY7fnGmO2GWOOjH32XO57iYhEo1PdA/x890n2Nnnp6hsGIrOdxnTW0APAX1tra4E1wF8ZY2qBR4CXrbWLgJfHrouIxAyfP8jLDW28sK8FgDuvKeWGqy872tMxl11Dt9aeBc6OXe41xjQA84HPAuvHNnsGeAX4HyGpUkTEAa09Pv54uodrKjysvSqP5MTIPo/kil4UNcZUAiuBN4HCsbAHaAUK57QyEREHDAwHOHNukIXzsqjMz+ArH6/CnR4dg2GmHejGmEzg18A3rbXnJ68fWWutMeaSr64aY+4D7gMoLw/N+eIiIrNlreVwWx87D7UTHLHMz0knLSUxasIcphnoxphkRsP8OWvteBOCNmNMsbX2rDGmGGi/1Ndaa58EnoTRs1zmoGYRkTnV6/Ozo7Gd4x39FLld3FxbSFpKotNlXbHpnOVigJ8CDdbaxybd9SJwz9jle4B/nfvywiMxMZEVK1awdOlS7rrrLgYGBmb8vV555RVuv/12AF588UUeffTRKbc9d+4cf/d3fzdx/cyZM9x5550z3reIXLmhQJDn3mzmVPcAN1xdwN31ZeRnpjpd1oxMZ4X/OuDPgU8YY94d+/g08ChwszHmCLBx7HpUSktL49133+XAgQOkpKTwk5/85IL7rbWMjIxc8fe94447eOSRqU/+uTjQS0pKeOGFF654PyJy5QaGRxvujTfT2rymgmsqPCQkRN7piNN12UC31r5mrTXW2jpr7Yqxj/+w1nZZazdYaxdZazdaa7vDUXCorVu3jqNHj9LU1ER1dTVf+tKXWLp0KadOneKll15i7dq1rFq1irvuuou+vj4Afvvb31JTU8OqVavYuvWDtphPP/00999/PzDaYvdzn/scy5cvZ/ny5bzxxhs88sgjHDt2jBUrVvDtb3+bpqYmli5dCoz2ivnKV77CsmXLWLlyJTt37pz4nps2beLWW29l0aJFEw2/gsEgX/7yl1m6dCnLli3j8ccfD+fTJhI1RkYs+056eeq1EzR19gOwpMRNTnpohtmEU8S99f+f95760G1XF2axvCwHf3CEf3nn9Ifury3JZkmJm8HhIP++/8wF991VXzbtfQcCAf7zP/+TW2+9FRhtiPXMM8+wZs0aOjs7+d73vsf27dvJyMjg+9//Po899hgPP/wwX/va19ixYwcLFy7k7rvvvuT3fvDBB7nxxhv5zW9+QzAYpK+vj0cffZQDBw5M9H5pamqa2P6JJ57AGMMf//hHGhsbueWWWzh8+DAw2ivmnXfeITU1lerqah544AHa29s5ffr0RNvec+fOTftxi8SLzrFmWq1jzbTyMqM/xCeLuEB3wuDg4MQouHXr1nHvvfdy5swZKioqWLNmDQC7d+/m4MGDEz1UhoeHWbt2LY2NjVRVVbFo0SIANm/ezJNPPvmhfezYsYNnn30WGF2zd7vdeL3eKWt67bXXeOCBBwAm+reMB/qGDRtwu0fbc9bW1nLy5EmWLFnC8ePHeeCBB7jtttu45ZZb5uKpEYkJ+1v385NdL9FwZoTctGy+vLqe25csish3e85GxAX6Rx1RJycmfOT9aSmJV3REPvF1Y2voF5vcPtday80338zzzz9/wTaX+rpQS0394AWbxMREAoEAHo+H9957j9/97nf85Cc/4Ve/+hVPPfVU2GsTiTTjk86Mv5zy3Dwysw7xz0feoCI/dF0PnRLZb3uKIGvWrOH111/n6NGjAPT393P48GFqampoamri2LFjAB8K/HEbNmzgxz/+MTC63t3T03NBe9yLrVu3jueeew6Aw4cP09zcTHV19ZT1dXZ2MjIywuc//3m+973v8fbbb8/4sYrEguHACL8/3MHf7/ktHpeHyoIkqop7KMh0h3QMnJMU6NNUUFDA008/zRe/+EXq6uomlltcLhdPPvkkt912G6tWrWLevHmX/Pof/vCH7Ny5k2XLlnHNNddw8OBB8vLyuO6661i6dCnf/va3L9j+61//OiMjIyxbtoy7776bp59++oIj84udPn2a9evXs2LFCjZv3szf/u3fzunjF4km48203j7p5ZS3G7fLzeTVlVBOOnOS2ufGET3XEut8/iB/ONLJgdM95KQns3FxIT/d/78/NOls/Pp313/XuWKvwJy1zxURiRatPT4OnjlPfaWHzWsqKMtNd3TSWbgp0EUkqg0MBzjSNvpaVGV+Bl/+eCXrFhVMdEYM9xg4J0XEWS7hnPwRr8K5tCYSDtZaGlt7+f3hDoIjllLP1M20wjkGzkmOB7rL5aKrq4u8vDyFeohYa+nq6sLlcjldisicOO/zs6OhnROd/RRHcTOtueZ4oJeWltLS0kJHR4fTpcQ0l8tFaWmp02WIzNpQIMhzu5sJjoxwY3UBK0pzorr/ylxyPNCTk5OpqqpyugwRiXD9QwEyUpNITUrkxqsLmJ+TFlW9ysNBL4qKSEQbGbHsbeq+oJlWbUm2wvwSHD9CFxGZSnuvj+0H22k772PhvEzys6KzT3m4KNBFJCK91dTNG0e7cCUncHtdMQvnZerEictQoItIRHIlJVJdlMWNVxfoDJZpUqCLSEQYDozwxrFO8jNTWTrfzbLS0Q+ZPgW6iDiuuWuAbQ1tnB/087HKXKfLiVoKdBFxjM8f5NXDHbx/5jye9GTuqi+l1JPudFlRS4EuIo5pO++j4WwvH6vM5doFuRP9V2RmFOgiEjb7W/fzTwf+hSMdXdQWe9hUs4kvX7cYd5rOKZ8L+nUoImHx3tn3+O72f+CdY1n4BxbT2dfDll1bONnT4HRpMUOBLiIh1zPoZ8vOV+nvXUhORjLVZZ3kx/AoOKdoyUVEQmooEOQXbzZz2jtATQkU5AxOjIOL1VFwTlGgi0hITG6mtb66gGb/MAOBToz5YBRcj6+Hcne5g1XGFi25iMicCo5Y3hprpnVirJnW4uJsvrjss3EzCs4pCnQRmTPt53388q1mXjvSSVVBBvMmNdOKp1FwTtGSi4jMiT0nutl1rIu0lNFmWosKsz60TbyMgnOKAl1E5kR6SiI1xaPNtFzJaqblBAW6iMzIcGCE14+ONtNaVupm6fzRD3GOAl1ErlhTZz/bG9roGwqomVYEUaCLyLT5/EFeOdRBw9nz5Gak8IX6Mkpy0pwuS8Yo0EVk2trO+zjU2su1VbmsrsolSc20IooCXUQ+Uv9QgBbvINVFWVTkZfCV6yvJdqmZViRSoIvIJVlrOXj2PL8/3IG1UJGXjis5UWEewRToIvIhPYN+Xm5o42TXAPM9ady8uFCnIkYBBbqIXGC8mdaItXyiZh51pW7MeDctiWgKdBEBoG8oQOZYM62bagooyUnT8kqUuexL1MaYp4wx7caYA5Nu+64x5rQx5t2xj0+HtkwRCZXgiOXN410XNNOqKcpWmEeh6RyhPw38CHj2otsft9ZumfOKRCSk9rfuZ2vjVpp7mslPvYpccxOpCflcXZhFYXbq5b+BRKzLHqFba18FusNQi4iE2P7W/WzZtQXvoJfkwGL2n8jkpWM7qZ7fy211xaSnaBU2ms3mXQH3G2P2jy3JeC6/uYg4bWvjVjwuD540DynJlpI8S21FJ3va/83p0mQOzDTQfwxcBawAzgI/mGpDY8x9xpi9xpi9HR0dM9ydiMzWUCDI203DBIaKAcjLHqB83jly07M0Bi5GzCjQrbVt1tqgtXYE+Htg9Uds+6S1tt5aW19QUDDTOkVkFk509vOPu06SEKjEOzB0wX0aAxc7ZhToxpjiSVc/BxyYalsRcc7gcJDfHmjlX945TUpSAg/eWE9S2jGNgYtRl30FxBjzPLAeyDfGtADfAdYbY1YAFmgC/msIaxSRGeroHeJwWy/XLshldWUuSYmV5Gc9NHGWS7m7nHtX3qspQjHCWGvDtrP6+nq7d+/esO1PJB71DQVo8Q5QU5QNQK/PT5bOKY9qxph91tr6y22nc5REYoS1lvfPnOfVI6PNtCrzMnAlJyrM44gCXSQG9Az42dbQxqnuAUo9adxcq2Za8UiBLhLlfP4gz+05ibWwcXEhS+dnq5lWnFKgi0Sp8bVxV3IiG2oKKclxaXklzml+lEiUCY5Ydh/v4mevN00006ouylKYi47QRaJJa4+PbQ1tdPYOUVOkZlpyIQW6SJTYfbyL3ce7yExN4o4VJVxVkOl0SRJhFOgiUSIzNYmlJW6uX5SvM1jkkhToIhHK5w/y+tFOCrJSqSvNYel8N0vnu50uSyKYAl0kAh3v6GNHYzt9QwGurcpzuhyJEgp0EYdMnhxU7i5nU80mFubW8vtDHTS29pKfmcLtdeUUuV1OlypRQoEu4oDxyUEel4fS7FK8g1627NrC5tpvcqQ9i7VX5fGxylwSE/QGIZk+nYcu4oDJk4MCwSRsoASPy8MbZ1/kq9dXsWZBnsJcrpiO0EUc0NzTzPysUjp70jnTOfpCZ03FAM09zWSm6r+lzIx+ckQcUJhWxYGTKYwEcshKH6Ks4Bz9/nOaHCSzoiUXkTDz+YMkD92Et3+EHHczVcUdDAQ7NTlIZk2BLhIm531+AFzJiWxevZxHP/MpquYlcbq3BU+ah4fWPqTJQTIrWnIRCbFAcIQ9Td3sbfJye10xCwoyubowC1jBmooVTpcnMUSBLhJCZ3sG2Xawja6+YRYXZ1HsTnO6JIlhCnSRENl1rIs3T4w20/qTlfOpys9wuiSJcQp0kRDJTkuirtTNdQvzSU1SMy0JPQW6yBzx+YO8dmS0mdbyshyWlLhZUqJmWhI+CnSROXCso48dDe30D6uZljhHgS4yCwPDAV451MGh1l7ys1K5Y0UJhdlqpiXOUKCLzEJn7zDH2vv4+FV51KuZljhMgS5yhc77/LR0D1Jbkk15Xjpfub5K/VckIuinUGSarLXsb+nhtaOdACwoyMCVnKgwl4ihn0SRafD2D7OtoY3T3kHKc9PZuLhQcz0l4ijQRS7D5w/yiz3NGAM31xaypCQbY7RWLpFHgS5x7VJj4MYbZPUM+nGnJeNKTuSW2kKKc9K0vCIRTd0WJW6Nj4HzDnovGAP3zpn3eONoJ0+/3sTxjj4AFhVmKcwl4inQJW5NHgOXYBLwpHlwUcSj217nzRPdVBepmZZEFx1ySNxq7mmmNLt04vrZrizauovpD3TxuZXzqVQzLYkyOkKXuFXuLqfH1zNxPSU5QFpaO2ur/QpziUoKdIlbty38E46cSeN4W5ARO0JC8hlSMo5wV+3nnC5NZEYU6BKXjrb38s7xLFbk3UZ6spuW8xoDJ9FPa+gSV/qHAuw81M6Rtj4KslL55ic+xrzsdU6XJTInFOgSV7r7hznR0c91C/O5psKjZloSUy675GKMecoY026MOTDptlxjzDZjzJGxz57Qlikycz2Dft4/M/riZ1luOl+9vorVVeqMKLFnOmvoTwO3XnTbI8DL1tpFwMtj10UiirWWd0+d4+e7T/L7wx34/EEAMvQGIYlRl/3Jtta+aoypvOjmzwLrxy4/A7wC/I85rEtkVrr7h9l+sI3T5wapzE/nEzVqpiWxb6aHKoXW2rNjl1uBwjmqR2TWfP4gz+9pJsEYbllSSG2xmmlJfJj1357WWmuMsVPdb4y5D7gPoLy8fLa7E5lSz4Afd/poM61PLimk2J2m5RWJKzM9D73NGFMMMPa5faoNrbVPWmvrrbX1BQUFM9ydyNQCwRFeO9LJ0280cWysmdbCeVkKc4k7Mw30F4F7xi7fA/zr3JQjcmVOnxvk57tP8lZTN4uLs5ifo2ZaEr8uewhjjHme0RdA840xLcB3gEeBXxlj7gVOAl8IZZEil/LG0U72NHWT5Upm06r5VOSp/4rEt+mc5fLFKe7aMMe1iEyLtRZjDDnpKSwvy+G6q/JJSVIXCxEtMkrU8PmDvHKogyK3ixVlOdSWZFNLttNliUQMBbpEhI8aBQdwpK2XHY3t+Pwj5GakOFipSOTS36niuKlGwe1v3U/fUIB/e+8M/77/LJmuJL54bRmrq3KdLlkkIukIXRw3eRQcMPF5a+NW7q1byMmuftYtymdVuYcE9V8RmZICXRx38Si4IX8igeFimocPTzTTSk/Rj6rI5WjJRRw3PgrOWug4l0Fj8zyOt6ZSklkBoDAXmSYFujhuU80m2noHeO9EOqc6sjGJ3XhyD/CFJRoFJ3IlFOjiuKvzlrDIdQ+JZOPKaGBZxRCPrPuGRsGJXCH9LSuOmdxM6541KyjJWaPlFZFZ0BG6hJ0/OMIfjnRc1EwrU2EuMkv6HyRh1eIdYPvBNrwDfpbOd6uZlsgcUqBL2Lx+tJM9J7pxpyXz+VWllOelO12SSExRoEvIjTfTys1IYVWFh7UL8tRMSyQEFOgSMoPDQX5/uJ3CbBcryz0sLs5mcbHTVYnELgW6zDlrLYfb+njlUDtDgRHyMlOdLkkkLijQZU71DQV4uaGN4x39FLldbFxcSEGWAl0kHBToMqe8/cOc6h7ghqvzWVmmZloi4aRAl1nrGfBzyjvA0vluynLTuff6BaSlJDpdlkjcUaDLjI2MWN45dY5dxzpJTEhg4bxMXMmJCnMRhyjQZUY6+4bYfrCNsz0+FhRk8ImaebiSFeQiTlKgy4TLjYEb5/MH+ae3TpGYYPjUsiKqC7MwRmvlIk7TuzsE+OgxcOO8/cMAuJITuXVpEV9aW0FNUbbCXCRCKNAFuHAMXIJJwJPmwePysLVxK/7gCK8e7uCZXR8007qqQM20RCKN/kcK8OExcABul5tDbZ38fPdJzg34qStVMy2RSKZAF2B0DJx30DsxoBngyNlEBvpH19DvvKaUslw10xKJZFpyEWB0DJzX58U76CU4MoJ30Muw7WRT3Qo2r6lQmItEAQW6AFBXVMf99f+dnp4FHDzTjyfNw3c2fI17rl1NcqJ+TESigZZcBGsth9p6eft4FqsKPs3Hr8qjvjLX6bJE5Aop0ONcr8/PjsZ2jnf0U+x2sbG2kHx1RxSJSgr0OHduwE+Ld5Abri5gZVmOmmmJRDEFehw6NzDMqe5BlpWONtP66nVV6r8iEgMU6HFktJmWlzeOdpGUmMCiQjXTEoklCvQ40dE7xLaDbbSdVzMtkVilQI8DPn+QX+09RVKC4ba6YhbNy1T/FZEYpECPYd7+YTwZKbiSE/nU0iKK3WlaXhGJYXrHSAwaDozw+4uaaS0oyFSYi8Q4HaHHmOauAbY3tNEz6Gd5mZtSj5ppicQLBXoM+cORDvY2efGkJ3NXfSmlHvVfEYknswp0Y0wT0AsEgYC1tn4uipIrY63FGENBVir1lR7WLMhT/xWRODQXR+g3WWs75+D7yJjpjoIbGA7wyqEOitwuVpV7qCnKpqbIgYJFJCLoMC7CTGcUnLWWhrPneeaNkxxt78Na62DFIhIpZnuEboGXjDEW+L/W2ifnoKa4NnkUHDDxeWvjVuqK6jjv87OjoZ0Tnf2U5LjYuLiQPDXTEhFmH+jXW2tPG2PmAduMMY3W2lcnb2CMuQ+4D6C8vHyWu4t9U42Ca+5pBuD8oJ/T5wZZX13A8lI10xKRD8xqycVae3rsczvwG2D1JbZ50lpbb62tLygomM3u4kK5u5weX88Ft3X0DpBODQClnnTuvb6KleUehbmIXGDGgW6MyTDGZI1fBm4BDsxVYfHq4lFwR1stB5vzyUv8OD5/EEA9WETkkmZzhF4IvGaMeQ/YA/w/a+1v56as+FVXVMdDax8iNaGAPUeT6e2dzxeWrefbN69RkIvIR5rxGrq19jiwfA5rkTFX5y2hIiWdhTWGm6rnsagwy+mSRCQK6J2iEaS7f5jcSc20SnLSdFQuItOm89AjwHBghJ2H2nl2VxNH2z9opqUwF5EroSN0h53s6md7Qzu9Pj/LS3Moy1UzLRGZGQW6g1493MG+k15yM1K4q76M+TkKcxGZOQW6A8abaRVmu1hdlcu1VbkkqZmWiMySAj2M+ocC7DzUTklOGqvKPVQXZVGNzmARkbmhQA8Day0Hz57n1cOdBIIjFLu1tCIic0+BHmI9g352NLbR1DnA/Jw0NtYWkpuR4nRZIhKDFOgh1uvzc+acj5tq5rG81I0x6r8iIqGhQA+B7v5hTnUPsLwsZ6KZls4pF5FQU6DPoeCIZd9JL7uPd5GSlEB1URau5ESFuYiEhQL9I0x3FBxA+3kfLx1so6N3iEWFmdxUPU9BLiJhpZOfpzCdUXDjfP4g/7yvhYHhAJ9ZXsztdSVkpOp3pYiEl1JnCpcbBQfQ1TdEXmYqruREPr2smGK3S0flIuIYHaFPobmnGbfLfcFt46PghgJBdja28+yukxPNtKryMxTmIuIoHaFPodxdjnfQO3FkDtDj6yEnaRH/uOskfUMBVpbnUJ6b7mCVIiIf0BH6FCaPghuxI3gHvRxrTSVl+HpSkhL4Qn0Z66vnkZKkp1BEIoOO0KcwPgru1w2jZ7lU5JTz31Z/htzUClarmZaIRCAF+kdY4KmlPjefOxakcU2F5/JfICLiIAX6JVhref/MeV490kEwaJnvUTMtEYl8CvSL9Az62X6wjebuAeZ70rh5cSEeNdMSkSigQL9I31CA1vM+PlEzjzo10xKRKKJAZ/QNQqe8g6woy2F+TpqaaYlIVIrrQA+OWN5q6mbPiW5SkxKoUTMtEYlicRvobWPNtDp7h6guymJ9dYGCXESiWlwGus8f5IV9LaQkJnDHihKuKsh0uiQRkVmLq0Dv7BsiLyMFV3Iity0rpkjNtEQkhsTF2x2HAkF2NLbxj7tOcqyjH4BKNdMSkRgT80foJzr7ebmhjb6hAKsqPGqmJSIxK+ID/UqmBl3slUPtvNN8jrzMFO6uK6PYrXd8ikjsiugllyuZGjTOWou1FoCSnDSuXZDLf1ldrjAXkZgX0YE+eWpQgknAk+bB4/KwtXHrJbfv9fl58b0zvN3sBeDqwiw+flW+OiOKSFyI6CWX5p5mSrNLL7htfGrQZNZaDpwebaZlraUiLyOcZYqIRIT1KrSMAAAE7klEQVSIDvSppgaVu8s/uD7gZ1tDG6e6Byj1pHFzbSE56WqmJSLxJ6LXIi41Ncjr87KpZtPENn3DAdp7fWxcXMid15QqzEUkbpnxFxDDob6+3u7du/eKvuZSZ7mUZFZzqnuAleWjR+5DgSCpSTqnXERikzFmn7W2/nLbRfSSC4yOghs/TTE4YtlzoptfvN9MalICi4uzcSUnKsxFRIiCQB/X2uNj28FWOvuGqSnK4kY10xIRucCs1tCNMbcaYw4ZY44aYx6Zq6Iu5vMH+fXbLQwFRrhjRQmfWlZMekrU/C4SEQmLGaeiMSYReAK4GWgB3jLGvGitPThXxY1zJSdye10xhdlqpiUiMpXZHKGvBo5aa49ba4eBXwKfnZuyPqwiT820REQ+ymwCfT5watL1lrHbRETEASE/D90Yc58xZq8xZm9HR0eodyciErdmE+ingbJJ10vHbruAtfZJa229tba+oKBgFrsTEZGPMptAfwtYZIypMsakAH8KvDg3ZYmIyJWa8Vku1tqAMeZ+4HdAIvCUtfb9OatMRESuyKxO5rbW/gfwH3NUi4iIzEJEN+cSEZHpU6CLiMSIsHZbNMZ0ACdn+OX5QOcclhMN9Jjjgx5zfJjNY66w1l72NMGwBvpsGGP2Tqd9ZCzRY44PeszxIRyPWUsuIiIxQoEuIhIjoinQn3S6AAfoMccHPeb4EPLHHDVr6CIi8tGi6QhdREQ+QlQEergmI0UKY0yZMWanMeagMeZ9Y8w3nK4pHIwxicaYd4wx/+50LeFgjMkxxrxgjGk0xjQYY9Y6XVOoGWO+NfYzfcAY87wxxuV0TXPNGPOUMabdGHNg0m25xphtxpgjY589odh3xAf6pMlInwJqgS8aY2qdrSrkAsBfW2trgTXAX8XBYwb4BtDgdBFh9EPgt9baGmA5Mf7YjTHzgQeBemvtUkZ7QP2ps1WFxNPArRfd9gjwsrV2EfDy2PU5F/GBTpgnI0UCa+1Za+3bY5d7Gf2PHtPDQ4wxpcBtwD84XUs4GGPcwA3ATwGstcPW2nPOVhUWSUCaMSYJSAfOOFzPnLPWvgp0X3TzZ4Fnxi4/A/xJKPYdDYEe15ORjDGVwErgTWcrCbn/AzwMjDhdSJhUAR3Az8aWmf7BGJPhdFGhZK09DWwBmoGzQI+19iVnqwqbQmvt2bHLrUBhKHYSDYEet4wxmcCvgW9aa887XU+oGGNuB9qttfucriWMkoBVwI+ttSuBfkL0Z3ikGFs3/iyjv8xKgAxjzGZnqwo/O3pqYUhOL4yGQJ/WZKRYY4xJZjTMn7PWbnW6nhC7DrjDGNPE6JLaJ4wxP3e2pJBrAVqsteN/eb3AaMDHso3ACWtth7XWD2wFPu5wTeHSZowpBhj73B6KnURDoMfdZCRjjGF0bbXBWvuY0/WEmrX2f1prS621lYz+++6w1sb0kZu1thU4ZYypHrtpA3DQwZLCoRlYY4xJH/sZ30CMvxA8yYvAPWOX7wH+NRQ7mdWAi3CI08lI1wF/DvzRGPPu2G3/a2ygiMSOB4Dnxg5UjgNfcbiekLLWvmmMeQF4m9Ezud4hBt8xaox5HlgP5BtjWoDvAI8CvzLG3Mtox9kvhGTfeqeoiEhsiIYlFxERmQYFuohIjFCgi4jECAW6iEiMUKCLiMQIBbqISIxQoIuIxAgFuohIjPj/cWhBjuYQw6cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07c1157748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear Figure\n",
    "plt.clf()\n",
    "\n",
    "# Get predictions\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "# Plot true data\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "\n",
    "# Legend and plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), 'awesome_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a Linear Regression Model with PyTorch (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: CREATE MODEL CLASS\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 2: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LinearRegressionModel(input_size, output_size)\n",
    "\n",
    "'''\n",
    "STEP 3: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5: TRAIN THE MODEL\n",
    "'''\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print (f'epoch {epoch}, loss {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU: 2 things must be on GPU\n",
    "- model\n",
    "- variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: CREATE MODEL CLASS\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 2: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LinearRegressionModel(input_size, output_size)\n",
    "\n",
    "#####################\n",
    "# USE GPU FOR MODEL #\n",
    "#####################\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "'''\n",
    "STEP 3: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5: TRAIN THE MODEL\n",
    "'''\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Convert numpy array to torch Variable\n",
    "    \n",
    "    #####################\n",
    "    # USE GPU FOR MODEL #\n",
    "    #####################\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print (f'epoch {epoch}, loss {loss.data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple **linear regression basics**\n",
    "    - $y = Ax + B$\n",
    "    - $y = 2x + 1$\n",
    "- **Example** of simple linear regression\n",
    "- **Aim** of linear regression\n",
    "    - Minimizing distance between the points and the line\n",
    "        - Calculate \"distance\" through MSE\n",
    "        - Calculate gradients\n",
    "        - Update parameters with parameters = parameters - learning_rate * gradients\n",
    "        - Slowly update parameters A and B model the linear relationship between y and x of the form y = 2x + 1\n",
    "- Built a linear regression **model** in **CPU and GPU**\n",
    "    - Step 1: Create Model Class\n",
    "    - Step 2: Instantiate Model Class\n",
    "    - Step 3: Instantiate Loss Class\n",
    "    - Step 4: Instantiate Optimizer Class\n",
    "    - Step 5: Train Model\n",
    "- Important things to be on **GPU**\n",
    "    - model\n",
    "    - variables\n",
    "- How to bring to **GPU**?\n",
    "    - model_name.cuda()\n",
    "    - variable_name.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
