{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. PyTorch로 구현하는 순방향 뉴럴 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 순방향 신경망에 대해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 로지스틱 회귀분석의 뉴럴 네트워크로의 전이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀분석 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/07-01.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 로지스틱 회귀분석의 문제점 **\n",
    "\n",
    "- **선형** 함수들은 잘 표현할 수 있다.\n",
    "    - $y = 2x + 3$\n",
    "    - $y = x_1 + x_2$\n",
    "    - $y = x_1 + 3x_2 + 4x_3$\n",
    "- **비선형** 함수들은 잘 표현할 수 없다.\n",
    "    - $y = 4x_1 + 2x_2^2 + 3x_3^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introducing a Non-linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 비선형 함수 자세히 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수: 숫자를 받고 & 수학적 연산을 수행한다\n",
    "- 비선형성의 흔한 종류\n",
    "    - ReLUs (Rectified Linear Units)\n",
    "    - Sigmoid\n",
    "    - Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid (Logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\sigma(x) = \\frac{1}{1+e^{-x}}$ where $x$ are logits\n",
    "- input number $\\rightarrow$ [0, 1]\n",
    "    - 큰 음수일수록 $\\rightarrow$ 0\n",
    "    - 큰 양수일수록 $\\rightarrow$ 1\n",
    "- Cons:\n",
    "    1. 활성화 지점들이 0과 1에 모여있고 **gradients** $\\approx$ 0\n",
    "        - 가중치 업데이트를 할 시그널이 발생하지 않는다 $\\rightarrow$  **학습 불가**\n",
    "        - 해결책: 가중치를 신중하게 초기화하여 이것을 예방한다.\n",
    "    2. 출력값들이 0 주변에 없다 (출력값이 0에서 1사이에 있으니)\n",
    "        - 만약 출력값이 항상 양이라면 $\\rightarrow$ gradients는 항상 양이거나 음일 것이다 $\\rightarrow$ **그라디언트 업데이트에 좋지 않다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "- tanh(x) = $2\\sigma(2x) - 1$ where $\\sigma$ means Sigmoid\n",
    "    - 스케일된 sigmoid 함수이다.\n",
    "- Input number $\\rightarrow$ [-1, 1]\n",
    "- Cons:\n",
    "    1. 활성화 지점들이 0과 1에 모여있고 **gradients** $\\approx$ 0\n",
    "        - 가중치를 업데이트할 시그널이 발생하지 않는다. $\\rightarrow$ **학습 불가**\n",
    "        - **해결책**: 가중치를 신중하게 초기화하여 이것을 예방한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f(x)$ = max$(0, x)$\n",
    "- Pros:\n",
    "    1. 수렴을 빠르게 한다 $\\rightarrow$ **학습이 빠르다**\n",
    "    2. **연산적으로 비용이 적게 든다** 반면 Sigmoid/Tanh 는 exponentials 가 들어간다.\n",
    "- Cons:\n",
    "    1. 많은 ReLU 요소들이 \"죽는다\" $\\rightarrow$ **gradients = 0** 이면, 파라미터들에 대한 업데이트가 없다.\n",
    "        - **해결책**: 학습률을 잘 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch로 순방향 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 A: 1 Hidden Layer Feedfoward Neural Network (Sigmoid Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: 데이터셋 로드\n",
    "- Step 2: 데이터셋 순환 가능하게 만들기\n",
    "- Step 3: 모델 클래스 만들기\n",
    "- Step 4: 모델 클래스 인스턴스화\n",
    "- Step 5: 손실 클래스 인스턴스화\n",
    "- Step 6: 최적화 클래스 인스턴스화\n",
    "- Step 7: 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: MNIST 학습 데이터셋 로드\n",
    "** 0에서 9까지의 이미지 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root = './data',\n",
    "                            train = True,\n",
    "                            transform = transforms.ToTensor(),\n",
    "                            download = True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root = './data',\n",
    "                           train = False,\n",
    "                           transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 데이터셋 순환 가능하게 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000 / 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: 모델 클래스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear function # LINEAR\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "        # Linear function (readout) # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 모델 클래스 인스턴스화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **입력층** 차원: **784**\n",
    "    - 이미지의 크기\n",
    "    - 28 $\\times$ 28 = 784\n",
    "- **출력층** 차원: **10**\n",
    "    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "- **은닉층** 차원: **100**\n",
    "    - 어떤 숫자든지 가능함\n",
    "    - 비슷한 용어\n",
    "        - 뉴런의 수\n",
    "        - 비선형 활성화 함수의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: 손실 클래스 인스턴스화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순방향 신경망: **Cross Entropy Loss**\n",
    "    - *로지스틱 회귀분석*: **Cross Entropy Loss**\n",
    "    - *선형 회귀분석*: **MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 수식\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}$\n",
    "        - $\\theta$: 파라미터들 (우리의 변수들)\n",
    "        - $\\eta$: 학습률 (얼마나 빠르게 학습하고 싶은가)\n",
    "        - $\\nabla_{\\theta}$: 파라미터들의 그라디언트들\n",
    "        \n",
    "    - 더욱 간단한 수식\n",
    "        - 파라미터들 = 파라미터들 - 학습률 * 파라미터들의 그라디언트들\n",
    "        - **iteration이 한번 돌 때마다, 우리는 모델들의 파라미터들을 업데이트 한다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터들 자세히 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x113cd9e08>\n"
     ]
    }
   ],
   "source": [
    "print (model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "# Hidden Layer Parameters\n",
    "print (list(model.parameters())[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Bias Parameters\n",
    "print (list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# FC 2 Parameters\n",
    "print (list(model.parameters())[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# FC 2 Bias Parameters\n",
    "print (list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: 모델 학습시키기\n",
    "- 과정\n",
    "    1. 입력값/라벨들을 변수화 시킨다\n",
    "    2. 그라디언트 버퍼를 비워준다\n",
    "    3. 입력값에 대한 출력값을 구한다\n",
    "    4. 손실을 구한다\n",
    "    5. 파라미터들에 관한 그라디언트들을 구한다\n",
    "    6. 그라디언트들을 이용하여 파라미터들을 업데이트 시킨다\n",
    "        - 파라미터들 = 파라미터들 - 학습률 * 파라미터들의 그라디언트들\n",
    "    7. 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.7069175243377686, Accuracy: 86.23\n",
      "Iteration: 1000, Loss: 0.47253838181495667, Accuracy: 89.28\n",
      "Iteration: 1500, Loss: 0.37439411878585815, Accuracy: 90.57\n",
      "Iteration: 2000, Loss: 0.40091001987457275, Accuracy: 91.16\n",
      "Iteration: 2500, Loss: 0.29052475094795227, Accuracy: 91.64\n",
      "Iteration: 3000, Loss: 0.38746798038482666, Accuracy: 92.01\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updationg parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * int(correct) / int(total)\n",
    "            \n",
    "            # Print Loss\n",
    "            print (f'Iteration: {iter}, Loss: {loss.item()}, Accuracy: {accuracy}')              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: 데이터셋 로드\n",
    "- Step 2: 데이터셋 순환 가능하게 만들기\n",
    "- **Step 3: 모델 클래스 만들기**\n",
    "- Step 4: 모델 클래스 인스턴스화\n",
    "- Step 5: 손실 클래스 인스턴스화\n",
    "- Step 6: 최적화 클래스 인스턴스화\n",
    "- Step 7: 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.30187758803367615, Accuracy: 90.4\n",
      "Iteration: 1000, Loss: 0.39137744903564453, Accuracy: 93.48\n",
      "Iteration: 1500, Loss: 0.22136268019676208, Accuracy: 94.68\n",
      "Iteration: 2000, Loss: 0.1438414305448532, Accuracy: 95.6\n",
      "Iteration: 2500, Loss: 0.1414695680141449, Accuracy: 96.16\n",
      "Iteration: 3000, Loss: 0.02799205295741558, Accuracy: 96.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform= transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root = './data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        # Linear funciton 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-Linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-Linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 3 (readout): 100 --> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function2\n",
    "        out = self.fc2(out)\n",
    "        # Non-Linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get prediction from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * int(correct) / int(total)\n",
    "            \n",
    "            # Print Loss\n",
    "            print (f'Iteration: {iter}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-04.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: 데이터셋 로드\n",
    "- Step 2: 데이터셋 순환 가능하게 만들기\n",
    "- **Step 3: 모델 클래스 만들기**\n",
    "- Step 4: 모델 클래스 인스턴스화\n",
    "- Step 5: 손실 클래스 인스턴스화\n",
    "- Step 6: 최적화 클래스 인스턴스화\n",
    "- Step 7: 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.25381138920783997, Accuracy: 89.61\n",
      "Iteration: 1000, Loss: 0.36012136936187744, Accuracy: 93.02\n",
      "Iteration: 1500, Loss: 0.0649011954665184, Accuracy: 94.99\n",
      "Iteration: 2000, Loss: 0.08892955631017685, Accuracy: 95.99\n",
      "Iteration: 2500, Loss: 0.11272620409727097, Accuracy: 96.56\n",
      "Iteration: 3000, Loss: 0.14828279614448547, Accuracy: 96.71\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform= transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root = './data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-Linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-Linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-Linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 4 (readout) : 100 -- > 100\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 3\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get prediction from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * int(correct) / int(total)\n",
    "            \n",
    "            # Print Loss\n",
    "            print (f'Iteration: {iter}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥 러닝\n",
    "- 뉴럴 넷을 확장하기 위한 두가지 방법\n",
    "    - 비선형 활성화 유닛들(뉴런들)을 늘리기 (100 => 200) \n",
    "    - 은닉층을 늘리기 (3 => 5)\n",
    "- 단점\n",
    "    - 보다 큰 데이터셋이 필요하다\n",
    "        - 차원의 저주\n",
    "    - 높은 정확도를 보장하지는 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch로 순방향 신경망 구축하기 (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/08-04.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU: GPU 옵션이 켜져야 하는 두 가지\n",
    "- model\n",
    "- variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Step 1: 데이터셋 로드\n",
    "- Step 2: 데이터셋 순환 가능하게 만들기\n",
    "- **Step 3: 모델 클래스 만들기**\n",
    "- Step 4: 모델 클래스 인스턴스화\n",
    "- Step 5: 손실 클래스 인스턴스화\n",
    "- Step 6: 최적화 클래스 인스턴스화\n",
    "- **Step 7: 모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500, Loss: 0.43487799167633057, Accuracy: 90.44\n",
      "Iteration: 1000, Loss: 0.31847378611564636, Accuracy: 93.13\n",
      "Iteration: 1500, Loss: 0.0910906195640564, Accuracy: 95.6\n",
      "Iteration: 2000, Loss: 0.10492780059576035, Accuracy: 96.23\n",
      "Iteration: 2500, Loss: 0.08852627128362656, Accuracy: 96.74\n",
      "Iteration: 3000, Loss: 0.12088696658611298, Accuracy: 96.78\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, 28*28).cuda())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * int(correct) / int(total)\n",
    "            \n",
    "            # Print Loss\n",
    "            print (f'Iteration: {iter}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **로지스틱 회귀분석 문제점들** 비선형 함수들 표현\n",
    "    - **비-선형** 함수들을 잘 표현하지 못함\n",
    "        - y = 4$x_1$ + 2$x_2^2$ + 3$x_3^3$\n",
    "        - y = $x_1x_2$\n",
    "- 뉴럴 네트워크를 생성하기 위해 **비-선형성**을 로지스틱 회귀분석에 소개하였다.\n",
    "- 비선형의 종류들\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "    - ReLU\n",
    "- 순방향 뉴럴 네트워크 **모델들**\n",
    "    - 모델 A: 1 hidden layers (**sigmoid** activation)\n",
    "    - 모델 B: 1 hidden layers (**tanh** activation)\n",
    "    - 모델 C: 1 hidden layers (**ReLU** activation)\n",
    "    - 모델 D: **2 hidden** layers (ReLU activation)\n",
    "    - 모델 E: **3 hidden** layers (ReLU activation)\n",
    "- **코드**에서의 모델들의 변형\n",
    "    - Step 3번만 수정한다\n",
    "- 모델의 **능력**을 확장시키는 방법\n",
    "    - 보다 더 많은 활성화 유닛들 (**뉴런들**)\n",
    "    - 더 많은 **은닉층**\n",
    "- 능력을 확장시키는 것에 관한 **단점**\n",
    "    - **데이터**가 더 많이 필요하다\n",
    "    - **정확도** 향상을 보장해주지는 못한다\n",
    "- **GPU** 코드\n",
    "    - GPU를 활용하기 위해 추가해야 할 두가지 옵션\n",
    "        - **model**\n",
    "        - **variable**\n",
    "    - **Step 4 & Step 7 **만 수정해 주면 된다\n",
    "- **7 Step** 모델 구현 요약\n",
    "    - Step 1: 데이터셋 로드\n",
    "    - Step 2: 데이터셋 순환 가능하게 만들기\n",
    "    - Step 3: 모델 클래스 만들기\n",
    "    - Step 4: 모델 클래스 인스턴스화\n",
    "    - Step 5: 손실 클래스 인스턴스화\n",
    "    - Step 6: 최적화 클래스 인스턴스화\n",
    "    - Step 7: 모델 학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
